name: 'GoSecretScan'
description: 'Fast, concurrent secret scanner that detects API keys, credentials, and security vulnerabilities in source code'
author: 'M1RL0K'

branding:
  icon: 'shield'
  color: 'red'

inputs:
  scan-path:
    description: 'Directory path to scan for secrets (default: current directory)'
    required: false
    default: '.'
  fail-on-secrets:
    description: 'Fail the workflow if secrets are found (default: true)'
    required: false
    default: 'true'
  enable-llm:
    description: 'Enable LLM-powered verification (requires Docker)'
    required: false
    default: 'false'
  model-path:
    description: 'Path to the Granite GGUF model (relative to repo by default)'
    required: false
    default: '.gosecretscanner/models/granite-4.0-micro.Q4_K_M.gguf'
  llm-endpoint:
    description: 'HTTP endpoint for llama.cpp server (defaults to localhost)'
    required: false
    default: ''
  llm-port:
    description: 'Host port to expose the llama.cpp server on'
    required: false
    default: '8080'
  llama-image:
    description: 'Docker image to launch llama.cpp server'
    required: false
    default: 'ghcr.io/ggerganov/llama.cpp:server'
  manage-llm-server:
    description: 'Start/stop the llama.cpp server inside this action (set to false when using an external service).'
    required: false
    default: 'true'

outputs:
  secrets-found:
    description: 'Number of secrets found during the scan'
    value: ${{ steps.scan.outputs.secrets-found }}
  scan-status:
    description: 'Status of the scan (success or failed)'
    value: ${{ steps.scan.outputs.status }}

runs:
  using: 'composite'
  steps:
    - name: Set up Go
      uses: actions/setup-go@v5
      with:
        go-version: '1.24.7'

    - name: Build GoSecretScanner
      shell: bash
      run: |
        cd ${{ github.action_path }}
        go build -o gosecretscanner main.go
        chmod +x gosecretscanner

    - name: Download Granite model (LLM)
      if: inputs.enable-llm == 'true'
      shell: bash
      working-directory: ${{ github.action_path }}
      run: ./scripts/download-models.sh

    - name: Start llama.cpp server
      if: inputs.enable-llm == 'true' && inputs.manage-llm-server == 'true'
      shell: bash
      working-directory: ${{ github.action_path }}
      env:
        MODEL_PATH_INPUT: ${{ inputs.model-path }}
        PORT: ${{ inputs.llm-port }}
        LLAMA_CPP_IMAGE: ${{ inputs.llama-image }}
        SERVER_PORT: ${{ inputs.llm-port }}
        HOST_NETWORK: "false"
        HOST_IP: "127.0.0.1"
      run: |
        MODEL_PATH="$MODEL_PATH_INPUT"
        if [[ "$MODEL_PATH" != /* ]]; then
          MODEL_PATH="${{ github.action_path }}/$MODEL_PATH"
        fi
        export MODEL_PATH
        export PORT
        export LLAMA_CPP_IMAGE
        export DETACH="true"
        export CONTAINER_NAME="gosecretscanner-llm"
        export SERVER_PORT
        export HOST_NETWORK
        export HOST_IP
        ./scripts/run-llama-server.sh

    - name: Wait for llama.cpp server
      if: inputs.enable-llm == 'true'
      shell: bash
      env:
        LLM_ENDPOINT_INPUT: ${{ inputs.llm-endpoint }}
        LLM_PORT: ${{ inputs.llm-port }}
      run: |
        ENDPOINT="$LLM_ENDPOINT_INPUT"
        if [ -z "$ENDPOINT" ]; then
          ENDPOINT="http://127.0.0.1:$LLM_PORT"
        fi
        ATTEMPTS=0
        MAX_ATTEMPTS=120
        SLEEP_SECS=5
        while [ $ATTEMPTS -lt $MAX_ATTEMPTS ]; do
          if curl -fsS "$ENDPOINT/v1/models" >/dev/null 2>&1 || \
             curl -fsS "$ENDPOINT/v1/chat/completions" -o /dev/null 2>&1 || \
             curl -fsS "$ENDPOINT/healthz" >/dev/null 2>&1; then
            echo "llama.cpp server is ready"
            exit 0
          fi
          ATTEMPTS=$((ATTEMPTS+1))
          echo "Waiting for llama.cpp server on $ENDPOINT (attempt $ATTEMPTS/$MAX_ATTEMPTS)..."
          sleep $SLEEP_SECS
        done
        echo "llama.cpp server did not become ready in time. Recent container logs:" >&2
        docker logs --tail 200 gosecretscanner-llm >&2 || true
        exit 1

    - name: Run Secret Scanner
      id: scan
      shell: bash
      working-directory: ${{ inputs.scan-path }}
      run: |
        echo "Running GoSecretScanv2 scanner..."

        SECRET_COUNT=0
        STATUS="success"

        SCAN_CMD=("${{ github.action_path }}/gosecretscanner")

        if [ "${{ inputs.enable-llm }}" == "true" ]; then
          MODEL_PATH="${{ inputs.model-path }}"
          if [[ "$MODEL_PATH" != /* ]]; then
            MODEL_PATH="${{ github.action_path }}/$MODEL_PATH"
          fi
          LLM_ENDPOINT="${{ inputs.llm-endpoint }}"
          if [ -z "$LLM_ENDPOINT" ]; then
            LLM_ENDPOINT="http://127.0.0.1:${{ inputs.llm-port }}"
          fi
          SCAN_CMD+=("--llm" "--model-path=$MODEL_PATH" "--llm-endpoint=$LLM_ENDPOINT")
        fi

        # Run the scanner and capture output
        if "${SCAN_CMD[@]}"; then
          echo "No secrets found."
          SECRET_COUNT=0
          STATUS="success"
        else
          exit_code=$?
          if [ $exit_code -eq 1 ]; then
            echo "Secrets detected!"
            SECRET_COUNT=$(grep -c "File:" output.log 2>/dev/null || echo "unknown")
            STATUS="failed"

            if [ "${{ inputs.fail-on-secrets }}" == "true" ]; then
              echo "Failing workflow due to detected secrets."
              exit 1
            else
              echo "Secrets found but not failing workflow (fail-on-secrets=false)"
            fi
          else
            echo "Scanner encountered an error (exit code: $exit_code)"
            STATUS="error"
            exit $exit_code
          fi
        fi

        echo "secrets-found=$SECRET_COUNT" >> $GITHUB_OUTPUT
        echo "status=$STATUS" >> $GITHUB_OUTPUT

    - name: Stop llama.cpp server
      if: inputs.enable-llm == 'true' && inputs.manage-llm-server == 'true' && always()
      shell: bash
      run: |
        docker rm -f gosecretscanner-llm >/dev/null 2>&1 || true
