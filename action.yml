name: 'GoSecretScan'
description: 'Fast secret scanner with 189+ patterns. Scans files & git history for API keys, tokens, and credentials.'
author: 'M1RL0K'

branding:
  icon: 'shield'
  color: 'red'

inputs:
  scan-path:
    description: 'Directory path to scan for secrets (default: current directory)'
    required: false
    default: '.'
  fail-on:
    description: 'Minimum confidence level to fail: low, medium, high, critical (default: low = fail on any secret)'
    required: false
    default: 'low'
  output-format:
    description: 'Output format: text, json, or sarif (default: text)'
    required: false
    default: 'text'
  sarif-file:
    description: 'Path to write SARIF output (for GitHub Security tab integration)'
    required: false
    default: ''
  no-git-history:
    description: 'Skip scanning git commit history (default: false, scans entire history)'
    required: false
    default: 'false'
  git-max-commits:
    description: 'Maximum commits to scan in git history (0 = all)'
    required: false
    default: '0'
  config:
    description: 'Path to config file (.gosecretscanner.json)'
    required: false
    default: ''
  baseline:
    description: 'Path to baseline file for suppressing known findings'
    required: false
    default: ''
  redact:
    description: 'Redact secret values in output (default: true)'
    required: false
    default: 'true'
  enable-llm:
    description: 'Enable LLM-powered verification for fewer false positives (requires Docker)'
    required: false
    default: 'false'
  model-path:
    description: 'Path to the Granite GGUF model (relative to repo by default)'
    required: false
    default: '.gosecretscanner/models/granite-4.0-micro-Q4_K_M.gguf'
  llm-endpoint:
    description: 'HTTP endpoint for llama.cpp server (defaults to localhost)'
    required: false
    default: ''
  llm-port:
    description: 'Host port to expose the llama.cpp server on'
    required: false
    default: '8080'
  llama-image:
    description: 'Docker image to launch llama.cpp server'
    required: false
    default: 'ghcr.io/ggerganov/llama.cpp:server'
  manage-llm-server:
    description: 'Start/stop the llama.cpp server inside this action (set to false when using an external service).'
    required: false
    default: 'true'

outputs:
  secrets-found:
    description: 'Number of secrets found during the scan'
    value: ${{ steps.scan.outputs.secrets-found }}
  scan-status:
    description: 'Status of the scan (success or failed)'
    value: ${{ steps.scan.outputs.status }}
  sarif-file:
    description: 'Path to SARIF output file (if generated)'
    value: ${{ steps.scan.outputs.sarif-file }}

runs:
  using: 'composite'
  steps:
    - name: Set up Go
      uses: actions/setup-go@v5
      with:
        go-version: '1.24.7'

    - name: Build GoSecretScanner
      shell: bash
      run: |
        cd ${{ github.action_path }}
        go build -o gosecretscanner main.go
        chmod +x gosecretscanner

    - name: Download Granite model (LLM)
      if: inputs.enable-llm == 'true'
      shell: bash
      working-directory: ${{ github.action_path }}
      run: ./scripts/download-models.sh

    - name: Start llama.cpp server
      if: inputs.enable-llm == 'true' && inputs.manage-llm-server == 'true'
      shell: bash
      working-directory: ${{ github.action_path }}
      env:
        PORT: ${{ inputs.llm-port }}
        LLAMA_CPP_IMAGE: ${{ inputs.llama-image }}
        SERVER_PORT: ${{ inputs.llm-port }}
        HOST_NETWORK: "false"
        HOST_IP: "127.0.0.1"
      run: |
        # Convert relative path to absolute (working-directory is already github.action_path)
        MODEL_PATH="${{ inputs.model-path }}"
        if [[ "$MODEL_PATH" != /* ]]; then
          MODEL_PATH="$(pwd)/$MODEL_PATH"
        fi
        export MODEL_PATH
        export PORT
        export LLAMA_CPP_IMAGE
        export DETACH="true"
        export CONTAINER_NAME="gosecretscanner-llm"
        export SERVER_PORT
        export HOST_NETWORK
        export HOST_IP
        export WAIT_READY="false"
        ./scripts/run-llama-server.sh

    - name: Wait for llama.cpp server
      if: inputs.enable-llm == 'true'
      shell: bash
      env:
        LLM_ENDPOINT_INPUT: ${{ inputs.llm-endpoint }}
        LLM_PORT: ${{ inputs.llm-port }}
      run: |
        ENDPOINT="$LLM_ENDPOINT_INPUT"
        if [ -z "$ENDPOINT" ]; then
          ENDPOINT="http://127.0.0.1:$LLM_PORT"
        fi

        ATTEMPTS=0
        MAX_ATTEMPTS=120
        SLEEP_SECS=5
        echo "Waiting for llama.cpp server at $ENDPOINT/health ..."
        while [ $ATTEMPTS -lt $MAX_ATTEMPTS ]; do
          if curl -fsS "$ENDPOINT/health" >/dev/null 2>&1; then
            echo "llama.cpp server is ready at $ENDPOINT"

            # Warmup: Send a test request to ensure model is fully loaded
            echo "Warming up model with test request..."
            if curl -fsS "$ENDPOINT/v1/chat/completions" \
              -H "Content-Type: application/json" \
              -d '{"messages":[{"role":"user","content":"test"}],"max_tokens":5}' \
              >/dev/null 2>&1; then
              echo "Model warmup successful"
            else
              echo "Warning: Model warmup failed, but continuing..."
            fi

            exit 0
          fi
          ATTEMPTS=$((ATTEMPTS+1))
          echo "Attempt $ATTEMPTS/$MAX_ATTEMPTS..."
          sleep $SLEEP_SECS
        done
        echo "llama.cpp server did not become ready in time. Recent container logs:" >&2
        docker logs --tail 200 gosecretscanner-llm >&2 || true
        exit 1

    - name: Run Secret Scanner
      id: scan
      shell: bash
      working-directory: ${{ inputs.scan-path }}
      run: |
        echo "ğŸ” GoSecretScan - Enterprise Secret Scanner"
        echo "==========================================="

        SCAN_CMD=("${{ github.action_path }}/gosecretscanner")

        # Core options
        SCAN_CMD+=("--fail-on=${{ inputs.fail-on }}")

        # Output format
        OUTPUT_FORMAT="${{ inputs.output-format }}"
        SARIF_FILE="${{ inputs.sarif-file }}"

        # If SARIF file requested, use sarif output
        if [ -n "$SARIF_FILE" ]; then
          OUTPUT_FORMAT="sarif"
        fi
        SCAN_CMD+=("--output=$OUTPUT_FORMAT")

        # Redaction
        if [ "${{ inputs.redact }}" == "false" ]; then
          SCAN_CMD+=("--redact=false")
        fi

        # Git history options
        if [ "${{ inputs.no-git-history }}" == "true" ]; then
          SCAN_CMD+=("--no-git-history")
        fi
        if [ "${{ inputs.git-max-commits }}" != "0" ]; then
          SCAN_CMD+=("--git-max-commits=${{ inputs.git-max-commits }}")
        fi

        # Config file
        if [ -n "${{ inputs.config }}" ]; then
          SCAN_CMD+=("--config=${{ inputs.config }}")
        fi

        # Baseline file
        if [ -n "${{ inputs.baseline }}" ]; then
          SCAN_CMD+=("--baseline=${{ inputs.baseline }}")
        fi

        # LLM verification
        if [ "${{ inputs.enable-llm }}" == "true" ]; then
          MODEL_PATH="${{ inputs.model-path }}"
          if [[ "$MODEL_PATH" != /* ]]; then
            MODEL_PATH="${{ github.action_path }}/${MODEL_PATH#./}"
          fi
          LLM_ENDPOINT="${{ inputs.llm-endpoint }}"
          if [ -z "$LLM_ENDPOINT" ]; then
            LLM_ENDPOINT="http://127.0.0.1:${{ inputs.llm-port }}"
          fi
          SCAN_CMD+=("--llm" "--model-path=$MODEL_PATH" "--llm-endpoint=$LLM_ENDPOINT")
        fi

        echo "Command: ${SCAN_CMD[*]}"
        echo ""

        # Run the scanner
        # When saving to file: stdout (JSON/SARIF) goes to file, stderr (status) shows in console
        set +e
        if [ -n "$SARIF_FILE" ]; then
          "${SCAN_CMD[@]}" > "$SARIF_FILE"
          exit_code=$?
          SECRET_COUNT=$(jq '.runs[0].results | length' "$SARIF_FILE" 2>/dev/null || echo "0")
          echo ""
          echo "ğŸ“Š Results: $SECRET_COUNT findings written to $SARIF_FILE"
          echo "sarif-file=$SARIF_FILE" >> $GITHUB_OUTPUT
        else
          OUTPUT=$("${SCAN_CMD[@]}" 2>&1)
          exit_code=$?
          echo "$OUTPUT"

          if [ "$OUTPUT_FORMAT" == "json" ]; then
            SECRET_COUNT=$(echo "$OUTPUT" | jq 'length' 2>/dev/null || echo "0")
          else
            SECRET_COUNT=$(echo "$OUTPUT" | grep -c "potential secrets" | head -1 || echo "0")
          fi
        fi
        set -e

        # Determine status based on exit code
        if [ $exit_code -eq 0 ]; then
          echo ""
          echo "âœ… No secrets found!"
          STATUS="success"
          SECRET_COUNT=0
        elif [ $exit_code -eq 1 ]; then
          echo ""
          echo "ğŸš¨ Secrets detected!"
          STATUS="failed"
        else
          echo ""
          echo "âŒ Scanner encountered an error (exit code: $exit_code)"
          STATUS="error"
          exit $exit_code
        fi

        echo "secrets-found=$SECRET_COUNT" >> $GITHUB_OUTPUT
        echo "status=$STATUS" >> $GITHUB_OUTPUT

        # Exit with failure if secrets found
        if [ $exit_code -eq 1 ]; then
          exit 1
        fi

    - name: Upload SARIF as artifact
      if: inputs.sarif-file != '' && always()
      uses: actions/upload-artifact@v4
      with:
        name: secret-scan-sarif-${{ github.job }}
        path: ${{ inputs.sarif-file }}
        overwrite: true
      continue-on-error: true

    - name: Stop llama.cpp server
      if: inputs.enable-llm == 'true' && inputs.manage-llm-server == 'true' && always()
      shell: bash
      run: |
        docker rm -f gosecretscanner-llm >/dev/null 2>&1 || true
